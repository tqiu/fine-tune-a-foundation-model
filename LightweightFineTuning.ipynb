{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LORA\n",
    "* Model: BERT\n",
    "* Evaluation approach: custom function to calculate accuracy\n",
    "* Fine-tuning dataset: TimKoornstra/financial-tweets-sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8b820",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d674e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68d61f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model_name = 'bert-base-uncased'\n",
    "dataset_name = 'TimKoornstra/financial-tweets-sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c216da5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweet', 'sentiment', 'url'],\n",
       "        num_rows: 38091\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "# The dataset comprises tweets related to financial markets, stocks, and economic discussions. \n",
    "# Each tweet is labeled with a sentiment value, where '1' denotes a positive sentiment, \n",
    "# '2' signifies a negative sentiment, and '0' indicates a neutral sentiment.\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ca492a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$BYND - JPMorgan reels in expectations on Beyo...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://huggingface.co/datasets/zeroshot/twitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$CCL $RCL - Nomura points to bookings weakness...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://huggingface.co/datasets/zeroshot/twitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$CX - Cemex cut at Credit Suisse, J.P. Morgan ...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://huggingface.co/datasets/zeroshot/twitt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment  \\\n",
       "0  $BYND - JPMorgan reels in expectations on Beyo...          2   \n",
       "1  $CCL $RCL - Nomura points to bookings weakness...          2   \n",
       "2  $CX - Cemex cut at Credit Suisse, J.P. Morgan ...          2   \n",
       "\n",
       "                                                 url  \n",
       "0  https://huggingface.co/datasets/zeroshot/twitt...  \n",
       "1  https://huggingface.co/datasets/zeroshot/twitt...  \n",
       "2  https://huggingface.co/datasets/zeroshot/twitt...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize dataset\n",
    "pd.DataFrame(dataset['train'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c0d33f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    17368\n",
       "0    12181\n",
       "2     8542\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset['train']['sentiment']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9e70cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweet', 'label'],\n",
       "        num_rows: 38091\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the data slightly\n",
    "dataset = dataset.rename_column(\"sentiment\", \"label\")\n",
    "dataset = dataset.remove_columns(\"url\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c9a8559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, val, and test sets\n",
    "dataset_split1 = dataset['train'].train_test_split(test_size=0.2, seed=42, stratify_by_column='label')\n",
    "dataset_split2 = dataset_split1['train'].train_test_split(test_size=0.2, seed=42, stratify_by_column='label')\n",
    "\n",
    "dataset_train = dataset_split2['train']\n",
    "dataset_val = dataset_split2['test']\n",
    "dataset_test = dataset_split1['test']  # holdout dataset to be compared with peft model \n",
    "\n",
    "assert len(dataset['train']) == len(dataset_train) + len(dataset_val) + len(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1207f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6095/6095 [00:02<00:00, 2155.31 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tweet', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 24377\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the train and validation dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset_train = dataset_train.map(\n",
    "    lambda x: tokenizer(x['tweet'], padding=True, truncation=True, return_tensors='pt'), \n",
    "    batched=True)\n",
    "dataset_val = dataset_val.map(\n",
    "    lambda x: tokenizer(x['tweet'], padding=True, truncation=True, return_tensors='pt'), \n",
    "    batched=True)\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af82fe4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4c4dad3",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model (fine-tune classification head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6314ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    logits, labels = np.array(logits), np.array(labels)\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf8aaf6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 440M/440M [00:02<00:00, 212MB/s]  \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d5b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model parameters\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f3f4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", \n",
    "    num_train_epochs=3, \n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0aaf054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4572' max='4572' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4572/4572 45:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>0.988220</td>\n",
       "      <td>0.537326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.981100</td>\n",
       "      <td>0.958472</td>\n",
       "      <td>0.553404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.971500</td>\n",
       "      <td>0.955107</td>\n",
       "      <td>0.556850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4572, training_loss=0.9905648452611941, metrics={'train_runtime': 2729.0845, 'train_samples_per_second': 26.797, 'train_steps_per_second': 1.675, 'total_flos': 1.891171216690472e+16, 'train_loss': 0.9905648452611941, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdb5e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d22cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning with Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PEFT Config\n",
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d4c908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 440M/440M [00:02<00:00, 211MB/s]  \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 299,526 || all params: 109,781,766 || trainable%: 0.27283765866910903\n"
     ]
    }
   ],
   "source": [
    "# Convert a Transformers Model into a PEFT Model\n",
    "lora_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "lora_model = get_peft_model(lora_model, peft_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a088dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "lora_training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", \n",
    "    num_train_epochs=1, \n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e613e2d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3048' max='3048' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3048/3048 27:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>0.641427</td>\n",
       "      <td>0.730763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3048, training_loss=0.7594995986758255, metrics={'train_runtime': 1671.4271, 'train_samples_per_second': 14.585, 'train_steps_per_second': 1.824, 'total_flos': 5943863519458596.0, 'train_loss': 0.7594995986758255, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define trainer\n",
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=lora_training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# Train the model\n",
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd98d7",
   "metadata": {},
   "source": [
    "## ⚠️ The lora model weights are saved in \"./lora_results\" folder, as specified in TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d3c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_trainer.save_model(\"./bert_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5dad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cecd8f3",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning with Qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb8aa762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f18bbdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Quantize a model\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "qlora_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3, quantization_config=config)\n",
    "qlora_model = prepare_model_for_kbit_training(qlora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d96b239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PEFT Config\n",
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50824b28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 299,526 || all params: 109,781,766 || trainable%: 0.27283765866910903\n"
     ]
    }
   ],
   "source": [
    "# Convert a Transformers Model into a PEFT Model\n",
    "qlora_model = get_peft_model(qlora_model, peft_config)\n",
    "qlora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a75da0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "# thanks to quantization, batch size can be increased without memory error\n",
    "qlora_training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", \n",
    "    num_train_epochs=1, \n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25e0e892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1524' max='1524' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1524/1524 42:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.684999</td>\n",
       "      <td>0.705824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1524, training_loss=0.8052296037749043, metrics={'train_runtime': 2545.5606, 'train_samples_per_second': 9.576, 'train_steps_per_second': 0.599, 'total_flos': 3182409719779932.0, 'train_loss': 0.8052296037749043, 'epoch': 1.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define trainer\n",
    "qlora_trainer = Trainer(\n",
    "    model=qlora_model,\n",
    "    args=qlora_training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# Train the model\n",
    "qlora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6009eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize predictions and labels\n",
    "df = pd.DataFrame(dataset_val)\n",
    "predictions = qlora_trainer.predict(dataset_val)\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcc44b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>$EEENF on the go 🚀🤙🏼 https://t.co/zl4na3aiqE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838</th>\n",
       "      <td>Some banking tightening up. Gas &amp;amp; oil extended or very busy but a few possible set ups. Eyeing a few semis including $NVDA $QCOM $ADI for possible short.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5818</th>\n",
       "      <td>Wants more\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>$FB (110.20) is starting to show some relative strength and signs of potential B/O on the daily.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364</th>\n",
       "      <td>Canadian National Laying Off Workers</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                              tweet  \\\n",
       "941                                                                                                                    $EEENF on the go 🚀🤙🏼 https://t.co/zl4na3aiqE   \n",
       "2838  Some banking tightening up. Gas &amp; oil extended or very busy but a few possible set ups. Eyeing a few semis including $NVDA $QCOM $ADI for possible short.   \n",
       "5818                                                                                                                                                   Wants more\\n   \n",
       "2606                                                               $FB (110.20) is starting to show some relative strength and signs of potential B/O on the daily.   \n",
       "5364                                                                                                                           Canadian National Laying Off Workers   \n",
       "\n",
       "      label  predicted_label  \n",
       "941       1                1  \n",
       "2838      2                1  \n",
       "5818      1                1  \n",
       "2606      1                1  \n",
       "5364      2                0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df[['tweet', 'label', 'predicted_label']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference and compare model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0423de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, AutoPeftModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68cb80ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# base model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./results/checkpoint-4572\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8448e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# lora model: load with AutoPeftModelForSequenceClassification class\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"./lora_results/checkpoint-3048\", num_labels=3)\n",
    "# lora_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "# lora_model = PeftModel.from_pretrained(lora_model, \"./lora_results/checkpoint-3048\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d65d832c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# qlora model: load with AutoPeftModelForSequenceClassification class\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "qlora_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"./qlora_results/checkpoint-1524\", \n",
    "    num_labels=3,\n",
    "    quantization_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8e25417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on test dataset\n",
    "encoding = tokenizer(dataset_test['tweet'][:500], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "labels = dataset_test['label'][:500]\n",
    "\n",
    "model.to('cuda')\n",
    "encoding.to('cuda')\n",
    "lora_model.to('cuda')\n",
    "qlora_model.to('cuda')\n",
    "with torch.no_grad():\n",
    "    output = model(**encoding)\n",
    "    lora_output = lora_model(**encoding)\n",
    "    qlora_output = qlora_model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2956bb0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance of BERT model with tuning of classification head: {'accuracy': 0.552}\n",
      "Performance of BERT model with PEFT (LORA): {'accuracy': 0.75}\n",
      "Performance of BERT model with PEFT (QLORA): {'accuracy': 0.734}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Performance of BERT model with tuning of classification head: {compute_metrics((output.logits.cpu(), labels))}\n",
    "Performance of BERT model with PEFT (LORA): {compute_metrics((lora_output.logits.cpu(), labels))}\n",
    "Performance of BERT model with PEFT (QLORA): {compute_metrics((qlora_output.logits.cpu(), labels))}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb52b6",
   "metadata": {},
   "source": [
    "* Fine-tuning models with LORA does give significant performance boost even when training for just 1 epoch. (I would like to train more, but I keep getting server errors in Udactiy workspace)\n",
    "* Quantization with lora leads to lower accuracy as expected, but it allows me to increase batch size per device to 16 or even 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
